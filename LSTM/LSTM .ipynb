{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COMSCI703 Group - LSTM .ipynb","provenance":[{"file_id":"1jV4tWBO6m4vhB3kJOaeVlWYMrIrdJjZY","timestamp":1623273091573}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5YUqhfXHS6WM"},"source":["#### GPU Check ####\n"," \n","!nvidia-smi -L"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo1vAM9lKeE-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623392458065,"user_tz":-120,"elapsed":255,"user":{"displayName":"Matthew Waiariki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzA3OC5FPL1mCZM49xYXgzD6LJ3N5smk4x2Dl6=s64","userId":"14698851766960954927"}},"outputId":"10a75a36-e6ae-4882-b5d9-fb705e061b8a"},"source":["#### Setup Kaggle ####\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Kaggle\"\n","%cd '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/'"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/COMPSCI703 - PG/Colab\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lQuEKcVsEiz6"},"source":["#### Downloading Files ####\n","\n","%cd '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/Kaggle/'\n","\n","\n","# ! kaggle datasets download -d iezepov/gensim-embeddings-dataset\n","\n","%cd '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/'\n","\n","\n","# ! wget 'https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/blob/master/ethos/hs_data/en_dataset_with_stop_words.csv'\n","# ! wget 'https://raw.githubusercontent.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/master/ethos/ethos_data/Ethos_Dataset_Multi_Label.csv'\n","# ! wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n","\n","\n","%cd '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/'\n","\n","# Clean-up if requred.\n","\n","# rm -rf\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbdKEfRZJglL"},"source":["#### Unzipping Files #####\n","\n","%cd '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/'\n","\n","! unzip \\*.zip  && rm *.zip\n","\n","%cd '/content/gdrive/My Drive/COMPSCI703 - PG/Colab/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvT89hUoxdY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623393605774,"user_tz":-120,"elapsed":143121,"user":{"displayName":"Matthew Waiariki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhzA3OC5FPL1mCZM49xYXgzD6LJ3N5smk4x2Dl6=s64","userId":"14698851766960954927"}},"outputId":"ee90c959-aedd-4591-b488-15e90c2bb721"},"source":["##### TODO\n","## 1 - FIX Glove embedding UTF-8 errors\n","## 2 - FIX Output prediction\n","## 3 - FIX Output prediciton .CSV\n","## 4 - FIX Unknown Erorror with Modelling leading to averaged predictions arross all IDs\n","  \n","\n","##### IMPORT BIN #####\n","\n","import gc\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from keras.models import Model\n","from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n","from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.preprocessing import text, sequence\n","from keras.callbacks import LearningRateScheduler\n","from keras.losses import binary_crossentropy\n","from keras import backend as K\n","\n","######################\n","\n","\n","##### PRE-TRAINED INPUTS #####\n","EMBEDDING_FILES = [\n","    '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/Fasttext/crawl-300d-2M.vec',\n","    # '/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/Kaggle/gensim-embeddings/glove.840B.300d.gensim.vectors.npy'\n","]\n","######################\n","\n","\n","##### Model Settings #####\n","NUM_MODELS = 2\n","BATCH_SIZE = 512\n","LSTM_UNITS = 128\n","DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n","EPOCHS = 4\n","MAX_LEN = 220\n","######################\n","\n","\n","##### DEF Party #####\n","# Gets the Coefficients for given word returning a \"flat32\"\n","def get_coefs(word, *arr):\n","    return word, np.asarray(arr, dtype='float32')\n","\n","\n","# Loads embeddings from array list in form of a \"Dict\"\n","def load_embeddings(path):\n","    with open(path) as f:\n","        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n","\n","# Builds a matrix of the embedding index returning \"embedding_matrix\" var skipping when flagged with Key Errors\n","def matrix_builder(word_index, path):\n","    embedding_index = load_embeddings(path)\n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    for word, i in word_index.items():\n","        try:\n","            embedding_matrix[i] = embedding_index[word]\n","        except KeyError:\n","            pass\n","    return embedding_matrix\n","\n","\n","# Easy easily set and return Loss rate for the cross entropy in form of \"binary_crossentropy\"\n","def loss_rate(y_true, y_pred):\n","    return binary_crossentropy(K.reshape(y_true[:, 0], (-1, 1)), y_pred) * y_true[:, 1]\n","\n","# Bob's Building model func, returning the \"model\"\n","def build_model(embedding_matrix, num_alt_targets, loss_weight):\n","\n","    # Words fed in form of of shapes -- then imposing bidirectional & spatial dropouts\n","    words = Input(shape=(MAX_LEN,))\n","    x = Embedding(*embedding_matrix.shape,\n","                  weights=[embedding_matrix], trainable=False)(words)\n","    x = SpatialDropout1D(0.3)(x)\n","    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n","    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n","\n","    # NN background mapping\n","    hidden = concatenate([\n","        GlobalMaxPooling1D()(x),\n","        GlobalAveragePooling1D()(x),\n","    ])\n","    hidden = add(\n","        [hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    hidden = add(\n","        [hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    \n","    # Result of sigmoid operation\n","    result = Dense(1, activation='sigmoid')(hidden)\n","    # alt_result resevered for multiple files of search (as seen in ethos_multilabel)\n","    alt_result = Dense(num_alt_targets, activation='sigmoid')(hidden)\n","    # model formed by Adam optimization algorithm -- as discribed in arXiv:1412.6980 [cs.LG]\n","    model = Model(inputs=words, outputs=[result, alt_result])\n","    model.compile(loss=[loss_rate, 'binary_crossentropy'],\n","                  loss_weights=[loss_weight, 1.0], optimizer='adam')\n","\n","    return model\n","\n","# Pre-pocess used to sanatize data for use in model. \n","def preprocess(data):\n","    ## Credit for prepocess goes to Gabriel Preda @ https://www.kaggle.com/gpreda via https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n","    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + \\\n","        '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","\n","    def clean_special_chars(text, punct):\n","        for p in punct:\n","            text = text.replace(p, ' ')\n","        return text\n","\n","    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n","    return data\n","\n","\n","def testing(predictions):\n","    print(predictions)\n","    \n","######################\n","\n","\n","##### INPUT DATASET #####\n","ethos_dataset = pd.read_csv(\"/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/ETHOS/Ethos_Dataset_Binary.csv\", sep=\";\", dtype={'comment':'str','isHate':'float'})\n","\n","train = ethos_dataset\n","test = ethos_dataset['comment']\n","\n","training_data = preprocess(train['comment'])\n","identity_columns = ['isHate']\n","#########################\n","\n","\n","##### Weights Settigns #####\n","# Overall weights\n","weights = np.ones((len(training_data),)) / 4\n","# Subgroup\n","weights += (train[identity_columns].fillna(0).values >=\n","            0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n","# Background Positive, Subgroup Negative\n","weights += (((train['isHate'].values >= 0.5).astype(bool).astype(np.int) +\n","             (train[identity_columns].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(bool).astype(np.int) / 4\n","# Background Negative, Subgroup Positive\n","weights += (((train['isHate'].values < 0.5).astype(bool).astype(np.int) +\n","             (train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int)) > 1).astype(bool).astype(np.int) / 4\n","loss_weight = 1.0 / weights.mean()\n","#########################\n","\n","\n","##### TESTINGG #####\n","main_testing_data = np.vstack(\n","    [(train['isHate'].values >= 0.5).astype(np.int), weights]).T\n","alt_testing_data = train[['isHate']].values\n","testing_data = preprocess(test)\n","\n","tokenizer = text.Tokenizer()\n","tokenizer.fit_on_texts(list(training_data) + list(testing_data))\n","\n","\n","training_data = tokenizer.texts_to_sequences(training_data)\n","testing_data = tokenizer.texts_to_sequences(testing_data)\n","training_data = sequence.pad_sequences(training_data, maxlen=MAX_LEN)\n","testing_data = sequence.pad_sequences(testing_data, maxlen=MAX_LEN)\n","#########################\n","\n","embedding_matrix = np.concatenate(\n","    [matrix_builder(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n","\n","\n","\n","#### MODELING WORK ####\n","# Use of Pickle to dumb memory to file -- reducing memory usage with large datasets.\n","# Model based on that found by Tanrei(nama) @ https://www.kaggle.com/tanreinama/ via https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution/\n","#  \n","with open('tmp.pickle', mode='wb') as f:\n","    pickle.dump(testing_data, f)  # use tmp file to reduce memory\n","\n","del identity_columns, weights, tokenizer, train, test, testing_data\n","gc.collect()\n","\n","\n","checkpoint_predictions = []\n","weights = []\n","\n","for model_idx in range(NUM_MODELS):\n","    model = build_model(\n","        embedding_matrix, alt_testing_data.shape[-1], loss_weight)\n","    for global_epoch in range(EPOCHS):\n","        model.fit(\n","            training_data,\n","            [main_testing_data, alt_testing_data],\n","            batch_size=BATCH_SIZE,\n","            epochs=1,\n","            verbose=1,\n","            callbacks=[\n","                LearningRateScheduler(\n","                    lambda epoch: 1e-3 * (0.6 ** global_epoch))\n","            ]\n","        )\n","        with open('tmp.pickle', mode='rb') as f:\n","            testing_data = pickle.load(f)  # use tmp file to reduce memory\n","        checkpoint_predictions.append(model.predict(\n","            testing_data, batch_size=2048)[0].flatten())\n","        del testing_data\n","        gc.collect()\n","        weights.append(2 ** global_epoch)\n","    del model\n","    gc.collect()\n","    \n","############################\n","\n","\n","predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n","\n","\n","## Re_initi dataset for printing and writing \n","ethos_dataset2 = pd.read_csv(\"/content/gdrive/MyDrive/COMPSCI703 - PG/Colab/Datasets/ETHOS/Ethos_Dataset_Binary.csv\",\n","                             sep=\";\", dtype={'comment': 'str', 'isHate': 'float'})\n","testset = ethos_dataset2['comment']\n","\n","\n","working_final = pd.DataFrame.from_dict({\n","    'id': str(\"\"),\n","    'prediction': predictions\n","})\n","\n","for x in testset:\n","    final = pd.DataFrame.from_dict({\n","        'id': x,\n","        'prediction': predictions\n","    })\n","    working_final.append(final)\n","\n","testing(predictions)\n","\n","final.to_csv('prediction.csv', index=False)\n","\n","# Gedaan"],"execution_count":48,"outputs":[{"output_type":"stream","text":["2/2 [==============================] - 4s 250ms/step - loss: 1.3691 - dense_42_loss: 0.2449 - dense_43_loss: 0.6860\n","2/2 [==============================] - 1s 255ms/step - loss: 1.3127 - dense_42_loss: 0.2325 - dense_43_loss: 0.6640\n","2/2 [==============================] - 0s 245ms/step - loss: 1.2793 - dense_42_loss: 0.2259 - dense_43_loss: 0.6491\n","2/2 [==============================] - 0s 244ms/step - loss: 1.2504 - dense_42_loss: 0.2200 - dense_43_loss: 0.6366\n","2/2 [==============================] - 4s 251ms/step - loss: 1.3776 - dense_46_loss: 0.2461 - dense_47_loss: 0.6910\n","2/2 [==============================] - 0s 243ms/step - loss: 1.3291 - dense_46_loss: 0.2380 - dense_47_loss: 0.6651\n","2/2 [==============================] - 0s 241ms/step - loss: 1.2979 - dense_46_loss: 0.2306 - dense_47_loss: 0.6547\n","2/2 [==============================] - 0s 245ms/step - loss: 1.2850 - dense_46_loss: 0.2285 - dense_47_loss: 0.6475\n","[0.5493085  0.56906547 0.65150362 0.55820562 0.5648252  0.64933557\n"," 0.65688896 0.65205853 0.58114767 0.67706198 0.68913924 0.63175967\n"," 0.63509474 0.62590588 0.5885163  0.71767389 0.70449015 0.62978065\n"," 0.6920278  0.61460235 0.80613761 0.76422913 0.68905533 0.53099745\n"," 0.65014579 0.60607123 0.61637787 0.6126469  0.55099254 0.63091412\n"," 0.71317287 0.56832383 0.64962908 0.59947742 0.527946   0.68464074\n"," 0.66781985 0.63647955 0.61131734 0.69991647 0.62962679 0.6662313\n"," 0.70274416 0.8333305  0.75393456 0.61134636 0.57187253 0.64203812\n"," 0.73946496 0.61010085 0.59993409 0.60778453 0.64224785 0.68172192\n"," 0.61011011 0.56986658 0.70862737 0.67010986 0.51561591 0.58736236\n"," 0.71637002 0.60347387 0.72936154 0.77571994 0.60405012 0.70883035\n"," 0.69134007 0.58963975 0.64441389 0.61895745 0.61701154 0.58375188\n"," 0.5877177  0.56750155 0.68033997 0.69395009 0.59538788 0.58759862\n"," 0.59560099 0.58698346 0.74652044 0.59950228 0.58796513 0.5843694\n"," 0.65583253 0.70355455 0.63563458 0.74990367 0.68788028 0.56921778\n"," 0.57383328 0.63817302 0.57473442 0.77223753 0.62527516 0.55467582\n"," 0.6163154  0.58338789 0.55919483 0.56084829 0.54013951 0.59925818\n"," 0.50171587 0.59429722 0.58297006 0.62744248 0.67621849 0.61217385\n"," 0.56845314 0.6831302  0.59147179 0.63418249 0.7462212  0.73194309\n"," 0.68483921 0.61013176 0.57777176 0.64443949 0.62686753 0.66318001\n"," 0.74750563 0.6595866  0.66727488 0.69134855 0.64698522 0.6200277\n"," 0.61620671 0.59487994 0.58986634 0.67902405 0.66501213 0.75846312\n"," 0.63317987 0.6360711  0.63917115 0.6037012  0.67017317 0.68606065\n"," 0.59490261 0.77374464 0.61781963 0.59973177 0.77810358 0.676103\n"," 0.70374647 0.65300941 0.69497879 0.52492287 0.6112008  0.60765407\n"," 0.60656661 0.62614289 0.63434904 0.58722388 0.56742532 0.62500289\n"," 0.63683107 0.59901365 0.59911043 0.55928247 0.59418079 0.63313733\n"," 0.56685422 0.61892726 0.64112521 0.65452261 0.65154796 0.63032796\n"," 0.59622288 0.63821679 0.55572391 0.56068808 0.65498148 0.58082011\n"," 0.64024335 0.5160756  0.58073466 0.55920278 0.58817976 0.61372285\n"," 0.60833088 0.67688737 0.580773   0.58169411 0.60090946 0.66609092\n"," 0.70167542 0.60783443 0.55863333 0.66056231 0.66898417 0.60777937\n"," 0.64865613 0.6832532  0.62262034 0.60811779 0.61025475 0.66969863\n"," 0.67986283 0.63995939 0.54106088 0.65653765 0.60809236 0.68198344\n"," 0.63331299 0.64645308 0.60253557 0.60155224 0.62251124 0.7235387\n"," 0.60808354 0.60617425 0.6283982  0.57259492 0.60297814 0.64868497\n"," 0.6176262  0.60714209 0.64711239 0.61479411 0.67399583 0.53835113\n"," 0.54294732 0.58325607 0.61987636 0.65411928 0.6683854  0.58598913\n"," 0.5957657  0.56542205 0.69228531 0.55666525 0.55549629 0.60844017\n"," 0.59558586 0.63083817 0.60264056 0.61120871 0.60598872 0.53087223\n"," 0.53602065 0.5622083  0.70043964 0.67616747 0.61390612 0.67728987\n"," 0.62768275 0.65265387 0.64487221 0.64835423 0.60922441 0.51900285\n"," 0.58661295 0.62727722 0.57449384 0.69223386 0.63155742 0.57159464\n"," 0.55080193 0.66495911 0.61854954 0.66952396 0.63344846 0.63914089\n"," 0.55742329 0.73931035 0.58975094 0.63551572 0.60195084 0.58371324\n"," 0.56988774 0.6058577  0.68140437 0.59537779 0.58886498 0.58247583\n"," 0.54994976 0.6201601  0.59799205 0.57006635 0.56967068 0.63067496\n"," 0.61392761 0.57767611 0.77396674 0.58977117 0.58008681 0.62020573\n"," 0.62048359 0.60271414 0.61873204 0.58422936 0.57651842 0.60989946\n"," 0.55626386 0.53909086 0.5725952  0.6274661  0.60090546 0.55412194\n"," 0.57616338 0.64406083 0.58833677 0.65467024 0.64378057 0.54027046\n"," 0.6036298  0.52642395 0.64251261 0.70117352 0.6338917  0.66551927\n"," 0.6025158  0.62663028 0.67767367 0.61038262 0.67269935 0.64846446\n"," 0.55835011 0.55896473 0.57661285 0.54922516 0.6201427  0.66835878\n"," 0.58395024 0.5661553  0.55686295 0.62931711 0.63198929 0.57393634\n"," 0.56870901 0.54129463 0.61438136 0.61023158 0.6041348  0.64727994\n"," 0.53174494 0.58040534 0.60750359 0.63327072 0.56136251 0.66873306\n"," 0.75104866 0.57629878 0.5832178  0.61512423 0.63425757 0.57614455\n"," 0.59823814 0.56389263 0.61171765 0.64215364 0.6322784  0.53918458\n"," 0.58740489 0.70754695 0.60890028 0.58982416 0.77591157 0.55574823\n"," 0.59691691 0.59725411 0.60891862 0.55998142 0.55365936 0.54308462\n"," 0.58013761 0.5628099  0.58976739 0.66099247 0.62201192 0.67290377\n"," 0.65376632 0.59856268 0.56871029 0.62341935 0.62484331 0.61408242\n"," 0.58897036 0.57536871 0.61627159 0.57798416 0.55025247 0.62854201\n"," 0.59067037 0.60197375 0.59510824 0.66142712 0.53646246 0.66207841\n"," 0.69469386 0.61523347 0.54943758 0.63272176 0.54929465 0.65666691\n"," 0.55277365 0.56601103 0.58529653 0.55958394 0.6133443  0.58805946\n"," 0.5816498  0.52573686 0.52902964 0.59126143 0.58777229 0.50763784\n"," 0.53104951 0.6418658  0.71243319 0.5782295  0.55635397 0.66492388\n"," 0.57699636 0.59796573 0.57593178 0.62377101 0.58602361 0.55842625\n"," 0.58367765 0.64887868 0.58387645 0.54212596 0.56279791 0.54708173\n"," 0.6289497  0.61410038 0.61159653 0.55295951 0.66552984 0.61968458\n"," 0.64141917 0.59786121 0.56311925 0.56958764 0.52262661 0.58377296\n"," 0.57561618 0.58692199 0.60916222 0.57054937 0.59606595 0.63345448\n"," 0.68379198 0.6431998  0.56942243 0.58666045 0.66366359 0.57705562\n"," 0.60037456 0.64283157 0.59471018 0.60014291 0.68987009 0.53671019\n"," 0.65222184 0.59761861 0.55207429 0.56092453 0.75396883 0.65078031\n"," 0.55208689 0.50093339 0.60216628 0.54324291 0.64046091 0.57128911\n"," 0.57534793 0.60944675 0.63761972 0.64743446 0.58699302 0.55906847\n"," 0.64471696 0.56304713 0.51755189 0.55353436 0.54066303 0.5360184\n"," 0.56101663 0.56679516 0.55629477 0.5666185  0.64100291 0.59243631\n"," 0.60054239 0.51755913 0.62993688 0.51766642 0.56273896 0.57793005\n"," 0.49984588 0.65317198 0.59890662 0.60538325 0.53612562 0.5776278\n"," 0.63831707 0.61332085 0.5929423  0.57635326 0.55029824 0.59328504\n"," 0.68517077 0.56780102 0.59596081 0.5820598  0.56907463 0.70517278\n"," 0.5536499  0.57198611 0.57781932 0.51522768 0.63143804 0.56138554\n"," 0.56329965 0.51878995 0.55676613 0.60417647 0.59633297 0.5480516\n"," 0.5524159  0.53742943 0.56063563 0.55565391 0.56485433 0.60171473\n"," 0.53753948 0.62432142 0.50361039 0.62472857 0.58209812 0.5556812\n"," 0.50516439 0.56482276 0.65225231 0.5894253  0.51911767 0.56158636\n"," 0.6304859  0.52724772 0.58787909 0.54721343 0.5230601  0.49668724\n"," 0.54552651 0.55479821 0.54311579 0.5137758  0.53475839 0.54787857\n"," 0.52115967 0.58313248 0.59961341 0.56258258 0.56793734 0.56569873\n"," 0.5275164  0.5356582  0.56352064 0.70775039 0.61227885 0.51834532\n"," 0.62019562 0.54241739 0.55727149 0.61388777 0.51858086 0.64129741\n"," 0.56700262 0.54881328 0.58971534 0.53907524 0.60377276 0.60644064\n"," 0.6069105  0.5424499  0.68504266 0.53670513 0.55963016 0.55213859\n"," 0.52014788 0.5691962  0.62485468 0.53955537 0.5983862  0.55754736\n"," 0.52192083 0.51099571 0.5262701  0.61664563 0.53037965 0.55934676\n"," 0.57176098 0.48264352 0.56477163 0.62224695 0.5348913  0.57870112\n"," 0.54519991 0.5357479  0.59069034 0.59790769 0.59217193 0.65528448\n"," 0.64920927 0.54779599 0.61316517 0.55713971 0.56863837 0.58017135\n"," 0.59404002 0.55241973 0.58903073 0.53044794 0.54789769 0.71846305\n"," 0.59725433 0.58335395 0.58405319 0.5599433  0.67286481 0.53877661\n"," 0.55744007 0.55376029 0.62293107 0.62641838 0.57044749 0.56366963\n"," 0.56303174 0.69008612 0.52990532 0.69001175 0.61124416 0.58397708\n"," 0.64260782 0.54536962 0.69184827 0.59043829 0.63413022 0.61409808\n"," 0.57516837 0.5538553  0.58161982 0.55992905 0.5739566  0.53801075\n"," 0.53409935 0.54785358 0.57684869 0.54233202 0.5097905  0.55121005\n"," 0.6163351  0.55943507 0.50976433 0.48761313 0.65485322 0.4881514\n"," 0.57385844 0.5849355  0.59242813 0.54349921 0.49243898 0.57382096\n"," 0.56002621 0.54929877 0.57145479 0.51040067 0.52872341 0.51186825\n"," 0.50772383 0.56385229 0.50013609 0.52934188 0.54944237 0.64345549\n"," 0.49167595 0.53778088 0.48598033 0.55933111 0.64718697 0.44696094\n"," 0.47561457 0.48828456 0.52508013 0.54994769 0.52414103 0.53187724\n"," 0.55172301 0.53517855 0.56901913 0.55267863 0.65437923 0.53877921\n"," 0.56677582 0.56670382 0.58426652 0.50017398 0.50185194 0.59393049\n"," 0.52262009 0.56751811 0.55639454 0.63727337 0.54612084 0.54777793\n"," 0.49382145 0.5044615  0.48218389 0.57853027 0.48519904 0.61743023\n"," 0.5873951  0.5229358  0.52375117 0.51255179 0.53868981 0.5633331\n"," 0.5413809  0.5705469  0.52110813 0.51121568 0.4944661  0.59745812\n"," 0.51819476 0.61393779 0.57776621 0.50937622 0.52427726 0.54191149\n"," 0.6072764  0.53966877 0.64283843 0.50702417 0.52903105 0.53925042\n"," 0.53416552 0.53664336 0.56468664 0.56381015 0.55886057 0.53945612\n"," 0.54383672 0.56852514 0.50222468 0.51720076 0.50854199 0.53345854\n"," 0.56058143 0.66873631 0.66486304 0.54259419 0.57239909 0.5691303\n"," 0.50825243 0.56042529 0.57942349 0.57437767 0.5866727  0.54678013\n"," 0.532984   0.5611416  0.50718594 0.59700968 0.51362273 0.52869992\n"," 0.56173887 0.45933466 0.5010101  0.50407528 0.57550139 0.50501816\n"," 0.54540488 0.55320727 0.51734693 0.49056545 0.51794069 0.6505733\n"," 0.5813823  0.57142312 0.60220265 0.55411934 0.52911097 0.53675654\n"," 0.50977899 0.51021979 0.54985403 0.59072873 0.57317796 0.58235253\n"," 0.53419752 0.57735324 0.59885556 0.51857569 0.56442986 0.49016996\n"," 0.51119434 0.51667562 0.49156262 0.49306491 0.52641214 0.5444234\n"," 0.5388477  0.58248128 0.55777721 0.50709055 0.5062739  0.56975626\n"," 0.51392716 0.55534758 0.53991559 0.57274383 0.53183964 0.51970127\n"," 0.5361467  0.54524293 0.55748118 0.48349205 0.54439809 0.61325929\n"," 0.5005747  0.51365848 0.56442505 0.56282781 0.54715358 0.50688884\n"," 0.54630934 0.68810735 0.61581893 0.4929626  0.57304092 0.55046301\n"," 0.58792711 0.62729355 0.53004494 0.48979823 0.51597959 0.57517022\n"," 0.54095354 0.52553888 0.54455613 0.56222986 0.55543265 0.51453186\n"," 0.56311925 0.53754088 0.51612136 0.56715923 0.55269233 0.52884339\n"," 0.57086513 0.57009225 0.52102818 0.64020664 0.53565464 0.51685965\n"," 0.5077612  0.49993704 0.51254911 0.48703113 0.52036446 0.49827571\n"," 0.54374656 0.53851405 0.57838137 0.58708645 0.50426884 0.54177393\n"," 0.52322006 0.5689538  0.49320815 0.64645307 0.58936264 0.52458567\n"," 0.51992535 0.48693486 0.49888732 0.51454161 0.55792637 0.52002266\n"," 0.53476847 0.54320251 0.58119294 0.52582208 0.55038513 0.53717737\n"," 0.54824291 0.54929264 0.61162735 0.48657538 0.51405489 0.53079979\n"," 0.53141738 0.52314855 0.647057   0.53031716 0.521938   0.56369499\n"," 0.51392069 0.47594313 0.52992802 0.52768354 0.59041756 0.57756279\n"," 0.52815715 0.55248452 0.54954534 0.50894005 0.49916833 0.51140943\n"," 0.55260042 0.49994823 0.53016151 0.50930358 0.52176206 0.56029194\n"," 0.50497622 0.5232534  0.51851427 0.53168203 0.5908706  0.56338681\n"," 0.57041351 0.67915736 0.54026762 0.57372474 0.51110515 0.48259903\n"," 0.49318834 0.55190776 0.51431991 0.51597945 0.50355185 0.561075\n"," 0.60843835 0.49672641 0.53384175 0.55368226 0.56153927 0.59921377\n"," 0.54769225 0.50228954 0.5132434  0.49022285 0.52391074 0.51964201\n"," 0.53155374 0.52115564 0.62635652 0.6178987  0.58525985 0.5986036\n"," 0.56044849 0.51259188 0.60922206 0.64669457 0.53143025 0.5099417\n"," 0.58947388 0.57435779 0.58170828 0.52868585 0.54811551 0.51742246\n"," 0.56364905 0.50439807 0.51458208 0.53920849 0.54289529 0.50018486\n"," 0.53956647 0.54382398 0.49931418 0.53056123 0.53862005 0.61287614\n"," 0.56507397 0.56235007 0.59442587 0.53998955 0.65113815 0.75655555\n"," 0.54380038 0.50828894 0.59120674 0.5382146  0.57884812 0.548989\n"," 0.52000392 0.59390284 0.51480076 0.54876089 0.53525742 0.52872019\n"," 0.51464487 0.52884474]\n"],"name":"stdout"}]}]}